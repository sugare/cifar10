# Generated by the protocol buffer compiler.  DO NOT EDIT!
# source: model_config.proto

import sys
_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode('latin1'))
from google.protobuf.internal import enum_type_wrapper
from google.protobuf import descriptor as _descriptor
from google.protobuf import message as _message
from google.protobuf import reflection as _reflection
from google.protobuf import symbol_database as _symbol_database
# @@protoc_insertion_point(imports)

_sym_db = _symbol_database.Default()




DESCRIPTOR = _descriptor.FileDescriptor(
  name='model_config.proto',
  package='nvidia.inferenceserver',
  syntax='proto3',
  serialized_options=None,
  serialized_pb=_b('\n\x12model_config.proto\x12\x16nvidia.inferenceserver\"\xb1\x01\n\x12ModelInstanceGroup\x12\x0c\n\x04name\x18\x01 \x01(\t\x12=\n\x04kind\x18\x04 \x01(\x0e\x32/.nvidia.inferenceserver.ModelInstanceGroup.Kind\x12\r\n\x05\x63ount\x18\x02 \x01(\x05\x12\x0c\n\x04gpus\x18\x03 \x03(\x05\"1\n\x04Kind\x12\r\n\tKIND_AUTO\x10\x00\x12\x0c\n\x08KIND_GPU\x10\x01\x12\x0c\n\x08KIND_CPU\x10\x02\"#\n\x12ModelTensorReshape\x12\r\n\x05shape\x18\x01 \x03(\x03\"\x92\x02\n\nModelInput\x12\x0c\n\x04name\x18\x01 \x01(\t\x12\x33\n\tdata_type\x18\x02 \x01(\x0e\x32 .nvidia.inferenceserver.DataType\x12\x39\n\x06\x66ormat\x18\x03 \x01(\x0e\x32).nvidia.inferenceserver.ModelInput.Format\x12\x0c\n\x04\x64ims\x18\x04 \x03(\x03\x12;\n\x07reshape\x18\x05 \x01(\x0b\x32*.nvidia.inferenceserver.ModelTensorReshape\";\n\x06\x46ormat\x12\x0f\n\x0b\x46ORMAT_NONE\x10\x00\x12\x0f\n\x0b\x46ORMAT_NHWC\x10\x01\x12\x0f\n\x0b\x46ORMAT_NCHW\x10\x02\"\xb3\x01\n\x0bModelOutput\x12\x0c\n\x04name\x18\x01 \x01(\t\x12\x33\n\tdata_type\x18\x02 \x01(\x0e\x32 .nvidia.inferenceserver.DataType\x12\x0c\n\x04\x64ims\x18\x03 \x03(\x03\x12;\n\x07reshape\x18\x05 \x01(\x0b\x32*.nvidia.inferenceserver.ModelTensorReshape\x12\x16\n\x0elabel_filename\x18\x04 \x01(\t\"\xb7\x02\n\x12ModelVersionPolicy\x12\x43\n\x06latest\x18\x01 \x01(\x0b\x32\x31.nvidia.inferenceserver.ModelVersionPolicy.LatestH\x00\x12=\n\x03\x61ll\x18\x02 \x01(\x0b\x32..nvidia.inferenceserver.ModelVersionPolicy.AllH\x00\x12G\n\x08specific\x18\x03 \x01(\x0b\x32\x33.nvidia.inferenceserver.ModelVersionPolicy.SpecificH\x00\x1a\x1e\n\x06Latest\x12\x14\n\x0cnum_versions\x18\x01 \x01(\r\x1a\x05\n\x03\x41ll\x1a\x1c\n\x08Specific\x12\x10\n\x08versions\x18\x01 \x03(\x03\x42\x0f\n\rpolicy_choice\"\x93\x02\n\x17ModelOptimizationPolicy\x12\x44\n\x05graph\x18\x01 \x01(\x0b\x32\x35.nvidia.inferenceserver.ModelOptimizationPolicy.Graph\x12O\n\x08priority\x18\x02 \x01(\x0e\x32=.nvidia.inferenceserver.ModelOptimizationPolicy.ModelPriority\x1a\x16\n\x05Graph\x12\r\n\x05level\x18\x01 \x01(\x05\"I\n\rModelPriority\x12\x14\n\x10PRIORITY_DEFAULT\x10\x00\x12\x10\n\x0cPRIORITY_MAX\x10\x01\x12\x10\n\x0cPRIORITY_MIN\x10\x02\"Z\n\x14ModelDynamicBatching\x12\x1c\n\x14preferred_batch_size\x18\x01 \x03(\x05\x12$\n\x1cmax_queue_delay_microseconds\x18\x02 \x01(\x04\"\xc1\x03\n\x15ModelSequenceBatching\x12&\n\x1emax_sequence_idle_microseconds\x18\x01 \x01(\x04\x12Q\n\rcontrol_input\x18\x02 \x03(\x0b\x32:.nvidia.inferenceserver.ModelSequenceBatching.ControlInput\x1a\xc6\x01\n\x07\x43ontrol\x12H\n\x04kind\x18\x01 \x01(\x0e\x32:.nvidia.inferenceserver.ModelSequenceBatching.Control.Kind\x12\x18\n\x10int32_false_true\x18\x02 \x03(\x05\x12\x17\n\x0f\x66p32_false_true\x18\x03 \x03(\x02\">\n\x04Kind\x12\x1a\n\x16\x43ONTROL_SEQUENCE_START\x10\x00\x12\x1a\n\x16\x43ONTROL_SEQUENCE_READY\x10\x01\x1a\x64\n\x0c\x43ontrolInput\x12\x0c\n\x04name\x18\x01 \x01(\t\x12\x46\n\x07\x63ontrol\x18\x02 \x03(\x0b\x32\x35.nvidia.inferenceserver.ModelSequenceBatching.Control\"\xed\x02\n\x0fModelEnsembling\x12:\n\x04step\x18\x01 \x03(\x0b\x32,.nvidia.inferenceserver.ModelEnsembling.Step\x1a\x9d\x02\n\x04Step\x12\x12\n\nmodel_name\x18\x01 \x01(\t\x12M\n\tinput_map\x18\x02 \x03(\x0b\x32:.nvidia.inferenceserver.ModelEnsembling.Step.InputMapEntry\x12O\n\noutput_map\x18\x03 \x03(\x0b\x32;.nvidia.inferenceserver.ModelEnsembling.Step.OutputMapEntry\x1a/\n\rInputMapEntry\x12\x0b\n\x03key\x18\x01 \x01(\t\x12\r\n\x05value\x18\x02 \x01(\t:\x02\x38\x01\x1a\x30\n\x0eOutputMapEntry\x12\x0b\n\x03key\x18\x01 \x01(\t\x12\r\n\x05value\x18\x02 \x01(\t:\x02\x38\x01\"&\n\x0eModelParameter\x12\x14\n\x0cstring_value\x18\x01 \x01(\t\"\xc0\x08\n\x0bModelConfig\x12\x0c\n\x04name\x18\x01 \x01(\t\x12\x10\n\x08platform\x18\x02 \x01(\t\x12\x42\n\x0eversion_policy\x18\x03 \x01(\x0b\x32*.nvidia.inferenceserver.ModelVersionPolicy\x12\x16\n\x0emax_batch_size\x18\x04 \x01(\x05\x12\x31\n\x05input\x18\x05 \x03(\x0b\x32\".nvidia.inferenceserver.ModelInput\x12\x33\n\x06output\x18\x06 \x03(\x0b\x32#.nvidia.inferenceserver.ModelOutput\x12\x45\n\x0coptimization\x18\x0c \x01(\x0b\x32/.nvidia.inferenceserver.ModelOptimizationPolicy\x12H\n\x10\x64ynamic_batching\x18\x0b \x01(\x0b\x32,.nvidia.inferenceserver.ModelDynamicBatchingH\x00\x12J\n\x11sequence_batching\x18\r \x01(\x0b\x32-.nvidia.inferenceserver.ModelSequenceBatchingH\x00\x12\x46\n\x13\x65nsemble_scheduling\x18\x0f \x01(\x0b\x32\'.nvidia.inferenceserver.ModelEnsemblingH\x00\x12\x42\n\x0einstance_group\x18\x07 \x03(\x0b\x32*.nvidia.inferenceserver.ModelInstanceGroup\x12\x1e\n\x16\x64\x65\x66\x61ult_model_filename\x18\x08 \x01(\t\x12U\n\x12\x63\x63_model_filenames\x18\t \x03(\x0b\x32\x39.nvidia.inferenceserver.ModelConfig.CcModelFilenamesEntry\x12H\n\x0bmetric_tags\x18\n \x03(\x0b\x32\x33.nvidia.inferenceserver.ModelConfig.MetricTagsEntry\x12G\n\nparameters\x18\x0e \x03(\x0b\x32\x33.nvidia.inferenceserver.ModelConfig.ParametersEntry\x1a\x37\n\x15\x43\x63ModelFilenamesEntry\x12\x0b\n\x03key\x18\x01 \x01(\t\x12\r\n\x05value\x18\x02 \x01(\t:\x02\x38\x01\x1a\x31\n\x0fMetricTagsEntry\x12\x0b\n\x03key\x18\x01 \x01(\t\x12\r\n\x05value\x18\x02 \x01(\t:\x02\x38\x01\x1aY\n\x0fParametersEntry\x12\x0b\n\x03key\x18\x01 \x01(\t\x12\x35\n\x05value\x18\x02 \x01(\x0b\x32&.nvidia.inferenceserver.ModelParameter:\x02\x38\x01\x42\x13\n\x11scheduling_choice*\xeb\x01\n\x08\x44\x61taType\x12\x10\n\x0cTYPE_INVALID\x10\x00\x12\r\n\tTYPE_BOOL\x10\x01\x12\x0e\n\nTYPE_UINT8\x10\x02\x12\x0f\n\x0bTYPE_UINT16\x10\x03\x12\x0f\n\x0bTYPE_UINT32\x10\x04\x12\x0f\n\x0bTYPE_UINT64\x10\x05\x12\r\n\tTYPE_INT8\x10\x06\x12\x0e\n\nTYPE_INT16\x10\x07\x12\x0e\n\nTYPE_INT32\x10\x08\x12\x0e\n\nTYPE_INT64\x10\t\x12\r\n\tTYPE_FP16\x10\n\x12\r\n\tTYPE_FP32\x10\x0b\x12\r\n\tTYPE_FP64\x10\x0c\x12\x0f\n\x0bTYPE_STRING\x10\rb\x06proto3')
)

_DATATYPE = _descriptor.EnumDescriptor(
  name='DataType',
  full_name='nvidia.inferenceserver.DataType',
  filename=None,
  file=DESCRIPTOR,
  values=[
    _descriptor.EnumValueDescriptor(
      name='TYPE_INVALID', index=0, number=0,
      serialized_options=None,
      type=None),
    _descriptor.EnumValueDescriptor(
      name='TYPE_BOOL', index=1, number=1,
      serialized_options=None,
      type=None),
    _descriptor.EnumValueDescriptor(
      name='TYPE_UINT8', index=2, number=2,
      serialized_options=None,
      type=None),
    _descriptor.EnumValueDescriptor(
      name='TYPE_UINT16', index=3, number=3,
      serialized_options=None,
      type=None),
    _descriptor.EnumValueDescriptor(
      name='TYPE_UINT32', index=4, number=4,
      serialized_options=None,
      type=None),
    _descriptor.EnumValueDescriptor(
      name='TYPE_UINT64', index=5, number=5,
      serialized_options=None,
      type=None),
    _descriptor.EnumValueDescriptor(
      name='TYPE_INT8', index=6, number=6,
      serialized_options=None,
      type=None),
    _descriptor.EnumValueDescriptor(
      name='TYPE_INT16', index=7, number=7,
      serialized_options=None,
      type=None),
    _descriptor.EnumValueDescriptor(
      name='TYPE_INT32', index=8, number=8,
      serialized_options=None,
      type=None),
    _descriptor.EnumValueDescriptor(
      name='TYPE_INT64', index=9, number=9,
      serialized_options=None,
      type=None),
    _descriptor.EnumValueDescriptor(
      name='TYPE_FP16', index=10, number=10,
      serialized_options=None,
      type=None),
    _descriptor.EnumValueDescriptor(
      name='TYPE_FP32', index=11, number=11,
      serialized_options=None,
      type=None),
    _descriptor.EnumValueDescriptor(
      name='TYPE_FP64', index=12, number=12,
      serialized_options=None,
      type=None),
    _descriptor.EnumValueDescriptor(
      name='TYPE_STRING', index=13, number=13,
      serialized_options=None,
      type=None),
  ],
  containing_type=None,
  serialized_options=None,
  serialized_start=3358,
  serialized_end=3593,
)
_sym_db.RegisterEnumDescriptor(_DATATYPE)

DataType = enum_type_wrapper.EnumTypeWrapper(_DATATYPE)
TYPE_INVALID = 0
TYPE_BOOL = 1
TYPE_UINT8 = 2
TYPE_UINT16 = 3
TYPE_UINT32 = 4
TYPE_UINT64 = 5
TYPE_INT8 = 6
TYPE_INT16 = 7
TYPE_INT32 = 8
TYPE_INT64 = 9
TYPE_FP16 = 10
TYPE_FP32 = 11
TYPE_FP64 = 12
TYPE_STRING = 13


_MODELINSTANCEGROUP_KIND = _descriptor.EnumDescriptor(
  name='Kind',
  full_name='nvidia.inferenceserver.ModelInstanceGroup.Kind',
  filename=None,
  file=DESCRIPTOR,
  values=[
    _descriptor.EnumValueDescriptor(
      name='KIND_AUTO', index=0, number=0,
      serialized_options=None,
      type=None),
    _descriptor.EnumValueDescriptor(
      name='KIND_GPU', index=1, number=1,
      serialized_options=None,
      type=None),
    _descriptor.EnumValueDescriptor(
      name='KIND_CPU', index=2, number=2,
      serialized_options=None,
      type=None),
  ],
  containing_type=None,
  serialized_options=None,
  serialized_start=175,
  serialized_end=224,
)
_sym_db.RegisterEnumDescriptor(_MODELINSTANCEGROUP_KIND)

_MODELINPUT_FORMAT = _descriptor.EnumDescriptor(
  name='Format',
  full_name='nvidia.inferenceserver.ModelInput.Format',
  filename=None,
  file=DESCRIPTOR,
  values=[
    _descriptor.EnumValueDescriptor(
      name='FORMAT_NONE', index=0, number=0,
      serialized_options=None,
      type=None),
    _descriptor.EnumValueDescriptor(
      name='FORMAT_NHWC', index=1, number=1,
      serialized_options=None,
      type=None),
    _descriptor.EnumValueDescriptor(
      name='FORMAT_NCHW', index=2, number=2,
      serialized_options=None,
      type=None),
  ],
  containing_type=None,
  serialized_options=None,
  serialized_start=479,
  serialized_end=538,
)
_sym_db.RegisterEnumDescriptor(_MODELINPUT_FORMAT)

_MODELOPTIMIZATIONPOLICY_MODELPRIORITY = _descriptor.EnumDescriptor(
  name='ModelPriority',
  full_name='nvidia.inferenceserver.ModelOptimizationPolicy.ModelPriority',
  filename=None,
  file=DESCRIPTOR,
  values=[
    _descriptor.EnumValueDescriptor(
      name='PRIORITY_DEFAULT', index=0, number=0,
      serialized_options=None,
      type=None),
    _descriptor.EnumValueDescriptor(
      name='PRIORITY_MAX', index=1, number=1,
      serialized_options=None,
      type=None),
    _descriptor.EnumValueDescriptor(
      name='PRIORITY_MIN', index=2, number=2,
      serialized_options=None,
      type=None),
  ],
  containing_type=None,
  serialized_options=None,
  serialized_start=1239,
  serialized_end=1312,
)
_sym_db.RegisterEnumDescriptor(_MODELOPTIMIZATIONPOLICY_MODELPRIORITY)

_MODELSEQUENCEBATCHING_CONTROL_KIND = _descriptor.EnumDescriptor(
  name='Kind',
  full_name='nvidia.inferenceserver.ModelSequenceBatching.Control.Kind',
  filename=None,
  file=DESCRIPTOR,
  values=[
    _descriptor.EnumValueDescriptor(
      name='CONTROL_SEQUENCE_START', index=0, number=0,
      serialized_options=None,
      type=None),
    _descriptor.EnumValueDescriptor(
      name='CONTROL_SEQUENCE_READY', index=1, number=1,
      serialized_options=None,
      type=None),
  ],
  containing_type=None,
  serialized_options=None,
  serialized_start=1692,
  serialized_end=1754,
)
_sym_db.RegisterEnumDescriptor(_MODELSEQUENCEBATCHING_CONTROL_KIND)


_MODELINSTANCEGROUP = _descriptor.Descriptor(
  name='ModelInstanceGroup',
  full_name='nvidia.inferenceserver.ModelInstanceGroup',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='name', full_name='nvidia.inferenceserver.ModelInstanceGroup.name', index=0,
      number=1, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=_b("").decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='kind', full_name='nvidia.inferenceserver.ModelInstanceGroup.kind', index=1,
      number=4, type=14, cpp_type=8, label=1,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='count', full_name='nvidia.inferenceserver.ModelInstanceGroup.count', index=2,
      number=2, type=5, cpp_type=1, label=1,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='gpus', full_name='nvidia.inferenceserver.ModelInstanceGroup.gpus', index=3,
      number=3, type=5, cpp_type=1, label=3,
      has_default_value=False, default_value=[],
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
    _MODELINSTANCEGROUP_KIND,
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=47,
  serialized_end=224,
)


_MODELTENSORRESHAPE = _descriptor.Descriptor(
  name='ModelTensorReshape',
  full_name='nvidia.inferenceserver.ModelTensorReshape',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='shape', full_name='nvidia.inferenceserver.ModelTensorReshape.shape', index=0,
      number=1, type=3, cpp_type=2, label=3,
      has_default_value=False, default_value=[],
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=226,
  serialized_end=261,
)


_MODELINPUT = _descriptor.Descriptor(
  name='ModelInput',
  full_name='nvidia.inferenceserver.ModelInput',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='name', full_name='nvidia.inferenceserver.ModelInput.name', index=0,
      number=1, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=_b("").decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='data_type', full_name='nvidia.inferenceserver.ModelInput.data_type', index=1,
      number=2, type=14, cpp_type=8, label=1,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='format', full_name='nvidia.inferenceserver.ModelInput.format', index=2,
      number=3, type=14, cpp_type=8, label=1,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='dims', full_name='nvidia.inferenceserver.ModelInput.dims', index=3,
      number=4, type=3, cpp_type=2, label=3,
      has_default_value=False, default_value=[],
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='reshape', full_name='nvidia.inferenceserver.ModelInput.reshape', index=4,
      number=5, type=11, cpp_type=10, label=1,
      has_default_value=False, default_value=None,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
    _MODELINPUT_FORMAT,
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=264,
  serialized_end=538,
)


_MODELOUTPUT = _descriptor.Descriptor(
  name='ModelOutput',
  full_name='nvidia.inferenceserver.ModelOutput',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='name', full_name='nvidia.inferenceserver.ModelOutput.name', index=0,
      number=1, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=_b("").decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='data_type', full_name='nvidia.inferenceserver.ModelOutput.data_type', index=1,
      number=2, type=14, cpp_type=8, label=1,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='dims', full_name='nvidia.inferenceserver.ModelOutput.dims', index=2,
      number=3, type=3, cpp_type=2, label=3,
      has_default_value=False, default_value=[],
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='reshape', full_name='nvidia.inferenceserver.ModelOutput.reshape', index=3,
      number=5, type=11, cpp_type=10, label=1,
      has_default_value=False, default_value=None,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='label_filename', full_name='nvidia.inferenceserver.ModelOutput.label_filename', index=4,
      number=4, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=_b("").decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=541,
  serialized_end=720,
)


_MODELVERSIONPOLICY_LATEST = _descriptor.Descriptor(
  name='Latest',
  full_name='nvidia.inferenceserver.ModelVersionPolicy.Latest',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='num_versions', full_name='nvidia.inferenceserver.ModelVersionPolicy.Latest.num_versions', index=0,
      number=1, type=13, cpp_type=3, label=1,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=950,
  serialized_end=980,
)

_MODELVERSIONPOLICY_ALL = _descriptor.Descriptor(
  name='All',
  full_name='nvidia.inferenceserver.ModelVersionPolicy.All',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=982,
  serialized_end=987,
)

_MODELVERSIONPOLICY_SPECIFIC = _descriptor.Descriptor(
  name='Specific',
  full_name='nvidia.inferenceserver.ModelVersionPolicy.Specific',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='versions', full_name='nvidia.inferenceserver.ModelVersionPolicy.Specific.versions', index=0,
      number=1, type=3, cpp_type=2, label=3,
      has_default_value=False, default_value=[],
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=989,
  serialized_end=1017,
)

_MODELVERSIONPOLICY = _descriptor.Descriptor(
  name='ModelVersionPolicy',
  full_name='nvidia.inferenceserver.ModelVersionPolicy',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='latest', full_name='nvidia.inferenceserver.ModelVersionPolicy.latest', index=0,
      number=1, type=11, cpp_type=10, label=1,
      has_default_value=False, default_value=None,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='all', full_name='nvidia.inferenceserver.ModelVersionPolicy.all', index=1,
      number=2, type=11, cpp_type=10, label=1,
      has_default_value=False, default_value=None,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='specific', full_name='nvidia.inferenceserver.ModelVersionPolicy.specific', index=2,
      number=3, type=11, cpp_type=10, label=1,
      has_default_value=False, default_value=None,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
  ],
  extensions=[
  ],
  nested_types=[_MODELVERSIONPOLICY_LATEST, _MODELVERSIONPOLICY_ALL, _MODELVERSIONPOLICY_SPECIFIC, ],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
    _descriptor.OneofDescriptor(
      name='policy_choice', full_name='nvidia.inferenceserver.ModelVersionPolicy.policy_choice',
      index=0, containing_type=None, fields=[]),
  ],
  serialized_start=723,
  serialized_end=1034,
)


_MODELOPTIMIZATIONPOLICY_GRAPH = _descriptor.Descriptor(
  name='Graph',
  full_name='nvidia.inferenceserver.ModelOptimizationPolicy.Graph',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='level', full_name='nvidia.inferenceserver.ModelOptimizationPolicy.Graph.level', index=0,
      number=1, type=5, cpp_type=1, label=1,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=1215,
  serialized_end=1237,
)

_MODELOPTIMIZATIONPOLICY = _descriptor.Descriptor(
  name='ModelOptimizationPolicy',
  full_name='nvidia.inferenceserver.ModelOptimizationPolicy',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='graph', full_name='nvidia.inferenceserver.ModelOptimizationPolicy.graph', index=0,
      number=1, type=11, cpp_type=10, label=1,
      has_default_value=False, default_value=None,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='priority', full_name='nvidia.inferenceserver.ModelOptimizationPolicy.priority', index=1,
      number=2, type=14, cpp_type=8, label=1,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
  ],
  extensions=[
  ],
  nested_types=[_MODELOPTIMIZATIONPOLICY_GRAPH, ],
  enum_types=[
    _MODELOPTIMIZATIONPOLICY_MODELPRIORITY,
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=1037,
  serialized_end=1312,
)


_MODELDYNAMICBATCHING = _descriptor.Descriptor(
  name='ModelDynamicBatching',
  full_name='nvidia.inferenceserver.ModelDynamicBatching',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='preferred_batch_size', full_name='nvidia.inferenceserver.ModelDynamicBatching.preferred_batch_size', index=0,
      number=1, type=5, cpp_type=1, label=3,
      has_default_value=False, default_value=[],
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='max_queue_delay_microseconds', full_name='nvidia.inferenceserver.ModelDynamicBatching.max_queue_delay_microseconds', index=1,
      number=2, type=4, cpp_type=4, label=1,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=1314,
  serialized_end=1404,
)


_MODELSEQUENCEBATCHING_CONTROL = _descriptor.Descriptor(
  name='Control',
  full_name='nvidia.inferenceserver.ModelSequenceBatching.Control',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='kind', full_name='nvidia.inferenceserver.ModelSequenceBatching.Control.kind', index=0,
      number=1, type=14, cpp_type=8, label=1,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='int32_false_true', full_name='nvidia.inferenceserver.ModelSequenceBatching.Control.int32_false_true', index=1,
      number=2, type=5, cpp_type=1, label=3,
      has_default_value=False, default_value=[],
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='fp32_false_true', full_name='nvidia.inferenceserver.ModelSequenceBatching.Control.fp32_false_true', index=2,
      number=3, type=2, cpp_type=6, label=3,
      has_default_value=False, default_value=[],
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
    _MODELSEQUENCEBATCHING_CONTROL_KIND,
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=1556,
  serialized_end=1754,
)

_MODELSEQUENCEBATCHING_CONTROLINPUT = _descriptor.Descriptor(
  name='ControlInput',
  full_name='nvidia.inferenceserver.ModelSequenceBatching.ControlInput',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='name', full_name='nvidia.inferenceserver.ModelSequenceBatching.ControlInput.name', index=0,
      number=1, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=_b("").decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='control', full_name='nvidia.inferenceserver.ModelSequenceBatching.ControlInput.control', index=1,
      number=2, type=11, cpp_type=10, label=3,
      has_default_value=False, default_value=[],
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=1756,
  serialized_end=1856,
)

_MODELSEQUENCEBATCHING = _descriptor.Descriptor(
  name='ModelSequenceBatching',
  full_name='nvidia.inferenceserver.ModelSequenceBatching',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='max_sequence_idle_microseconds', full_name='nvidia.inferenceserver.ModelSequenceBatching.max_sequence_idle_microseconds', index=0,
      number=1, type=4, cpp_type=4, label=1,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='control_input', full_name='nvidia.inferenceserver.ModelSequenceBatching.control_input', index=1,
      number=2, type=11, cpp_type=10, label=3,
      has_default_value=False, default_value=[],
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
  ],
  extensions=[
  ],
  nested_types=[_MODELSEQUENCEBATCHING_CONTROL, _MODELSEQUENCEBATCHING_CONTROLINPUT, ],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=1407,
  serialized_end=1856,
)


_MODELENSEMBLING_STEP_INPUTMAPENTRY = _descriptor.Descriptor(
  name='InputMapEntry',
  full_name='nvidia.inferenceserver.ModelEnsembling.Step.InputMapEntry',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='key', full_name='nvidia.inferenceserver.ModelEnsembling.Step.InputMapEntry.key', index=0,
      number=1, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=_b("").decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='value', full_name='nvidia.inferenceserver.ModelEnsembling.Step.InputMapEntry.value', index=1,
      number=2, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=_b("").decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  serialized_options=_b('8\001'),
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=2127,
  serialized_end=2174,
)

_MODELENSEMBLING_STEP_OUTPUTMAPENTRY = _descriptor.Descriptor(
  name='OutputMapEntry',
  full_name='nvidia.inferenceserver.ModelEnsembling.Step.OutputMapEntry',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='key', full_name='nvidia.inferenceserver.ModelEnsembling.Step.OutputMapEntry.key', index=0,
      number=1, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=_b("").decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='value', full_name='nvidia.inferenceserver.ModelEnsembling.Step.OutputMapEntry.value', index=1,
      number=2, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=_b("").decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  serialized_options=_b('8\001'),
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=2176,
  serialized_end=2224,
)

_MODELENSEMBLING_STEP = _descriptor.Descriptor(
  name='Step',
  full_name='nvidia.inferenceserver.ModelEnsembling.Step',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='model_name', full_name='nvidia.inferenceserver.ModelEnsembling.Step.model_name', index=0,
      number=1, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=_b("").decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='input_map', full_name='nvidia.inferenceserver.ModelEnsembling.Step.input_map', index=1,
      number=2, type=11, cpp_type=10, label=3,
      has_default_value=False, default_value=[],
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='output_map', full_name='nvidia.inferenceserver.ModelEnsembling.Step.output_map', index=2,
      number=3, type=11, cpp_type=10, label=3,
      has_default_value=False, default_value=[],
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
  ],
  extensions=[
  ],
  nested_types=[_MODELENSEMBLING_STEP_INPUTMAPENTRY, _MODELENSEMBLING_STEP_OUTPUTMAPENTRY, ],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=1939,
  serialized_end=2224,
)

_MODELENSEMBLING = _descriptor.Descriptor(
  name='ModelEnsembling',
  full_name='nvidia.inferenceserver.ModelEnsembling',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='step', full_name='nvidia.inferenceserver.ModelEnsembling.step', index=0,
      number=1, type=11, cpp_type=10, label=3,
      has_default_value=False, default_value=[],
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
  ],
  extensions=[
  ],
  nested_types=[_MODELENSEMBLING_STEP, ],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=1859,
  serialized_end=2224,
)


_MODELPARAMETER = _descriptor.Descriptor(
  name='ModelParameter',
  full_name='nvidia.inferenceserver.ModelParameter',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='string_value', full_name='nvidia.inferenceserver.ModelParameter.string_value', index=0,
      number=1, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=_b("").decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=2226,
  serialized_end=2264,
)


_MODELCONFIG_CCMODELFILENAMESENTRY = _descriptor.Descriptor(
  name='CcModelFilenamesEntry',
  full_name='nvidia.inferenceserver.ModelConfig.CcModelFilenamesEntry',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='key', full_name='nvidia.inferenceserver.ModelConfig.CcModelFilenamesEntry.key', index=0,
      number=1, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=_b("").decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='value', full_name='nvidia.inferenceserver.ModelConfig.CcModelFilenamesEntry.value', index=1,
      number=2, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=_b("").decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  serialized_options=_b('8\001'),
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=3137,
  serialized_end=3192,
)

_MODELCONFIG_METRICTAGSENTRY = _descriptor.Descriptor(
  name='MetricTagsEntry',
  full_name='nvidia.inferenceserver.ModelConfig.MetricTagsEntry',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='key', full_name='nvidia.inferenceserver.ModelConfig.MetricTagsEntry.key', index=0,
      number=1, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=_b("").decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='value', full_name='nvidia.inferenceserver.ModelConfig.MetricTagsEntry.value', index=1,
      number=2, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=_b("").decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  serialized_options=_b('8\001'),
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=3194,
  serialized_end=3243,
)

_MODELCONFIG_PARAMETERSENTRY = _descriptor.Descriptor(
  name='ParametersEntry',
  full_name='nvidia.inferenceserver.ModelConfig.ParametersEntry',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='key', full_name='nvidia.inferenceserver.ModelConfig.ParametersEntry.key', index=0,
      number=1, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=_b("").decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='value', full_name='nvidia.inferenceserver.ModelConfig.ParametersEntry.value', index=1,
      number=2, type=11, cpp_type=10, label=1,
      has_default_value=False, default_value=None,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  serialized_options=_b('8\001'),
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=3245,
  serialized_end=3334,
)

_MODELCONFIG = _descriptor.Descriptor(
  name='ModelConfig',
  full_name='nvidia.inferenceserver.ModelConfig',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='name', full_name='nvidia.inferenceserver.ModelConfig.name', index=0,
      number=1, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=_b("").decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='platform', full_name='nvidia.inferenceserver.ModelConfig.platform', index=1,
      number=2, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=_b("").decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='version_policy', full_name='nvidia.inferenceserver.ModelConfig.version_policy', index=2,
      number=3, type=11, cpp_type=10, label=1,
      has_default_value=False, default_value=None,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='max_batch_size', full_name='nvidia.inferenceserver.ModelConfig.max_batch_size', index=3,
      number=4, type=5, cpp_type=1, label=1,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='input', full_name='nvidia.inferenceserver.ModelConfig.input', index=4,
      number=5, type=11, cpp_type=10, label=3,
      has_default_value=False, default_value=[],
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='output', full_name='nvidia.inferenceserver.ModelConfig.output', index=5,
      number=6, type=11, cpp_type=10, label=3,
      has_default_value=False, default_value=[],
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='optimization', full_name='nvidia.inferenceserver.ModelConfig.optimization', index=6,
      number=12, type=11, cpp_type=10, label=1,
      has_default_value=False, default_value=None,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='dynamic_batching', full_name='nvidia.inferenceserver.ModelConfig.dynamic_batching', index=7,
      number=11, type=11, cpp_type=10, label=1,
      has_default_value=False, default_value=None,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='sequence_batching', full_name='nvidia.inferenceserver.ModelConfig.sequence_batching', index=8,
      number=13, type=11, cpp_type=10, label=1,
      has_default_value=False, default_value=None,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='ensemble_scheduling', full_name='nvidia.inferenceserver.ModelConfig.ensemble_scheduling', index=9,
      number=15, type=11, cpp_type=10, label=1,
      has_default_value=False, default_value=None,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='instance_group', full_name='nvidia.inferenceserver.ModelConfig.instance_group', index=10,
      number=7, type=11, cpp_type=10, label=3,
      has_default_value=False, default_value=[],
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='default_model_filename', full_name='nvidia.inferenceserver.ModelConfig.default_model_filename', index=11,
      number=8, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=_b("").decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='cc_model_filenames', full_name='nvidia.inferenceserver.ModelConfig.cc_model_filenames', index=12,
      number=9, type=11, cpp_type=10, label=3,
      has_default_value=False, default_value=[],
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='metric_tags', full_name='nvidia.inferenceserver.ModelConfig.metric_tags', index=13,
      number=10, type=11, cpp_type=10, label=3,
      has_default_value=False, default_value=[],
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='parameters', full_name='nvidia.inferenceserver.ModelConfig.parameters', index=14,
      number=14, type=11, cpp_type=10, label=3,
      has_default_value=False, default_value=[],
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
  ],
  extensions=[
  ],
  nested_types=[_MODELCONFIG_CCMODELFILENAMESENTRY, _MODELCONFIG_METRICTAGSENTRY, _MODELCONFIG_PARAMETERSENTRY, ],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
    _descriptor.OneofDescriptor(
      name='scheduling_choice', full_name='nvidia.inferenceserver.ModelConfig.scheduling_choice',
      index=0, containing_type=None, fields=[]),
  ],
  serialized_start=2267,
  serialized_end=3355,
)

_MODELINSTANCEGROUP.fields_by_name['kind'].enum_type = _MODELINSTANCEGROUP_KIND
_MODELINSTANCEGROUP_KIND.containing_type = _MODELINSTANCEGROUP
_MODELINPUT.fields_by_name['data_type'].enum_type = _DATATYPE
_MODELINPUT.fields_by_name['format'].enum_type = _MODELINPUT_FORMAT
_MODELINPUT.fields_by_name['reshape'].message_type = _MODELTENSORRESHAPE
_MODELINPUT_FORMAT.containing_type = _MODELINPUT
_MODELOUTPUT.fields_by_name['data_type'].enum_type = _DATATYPE
_MODELOUTPUT.fields_by_name['reshape'].message_type = _MODELTENSORRESHAPE
_MODELVERSIONPOLICY_LATEST.containing_type = _MODELVERSIONPOLICY
_MODELVERSIONPOLICY_ALL.containing_type = _MODELVERSIONPOLICY
_MODELVERSIONPOLICY_SPECIFIC.containing_type = _MODELVERSIONPOLICY
_MODELVERSIONPOLICY.fields_by_name['latest'].message_type = _MODELVERSIONPOLICY_LATEST
_MODELVERSIONPOLICY.fields_by_name['all'].message_type = _MODELVERSIONPOLICY_ALL
_MODELVERSIONPOLICY.fields_by_name['specific'].message_type = _MODELVERSIONPOLICY_SPECIFIC
_MODELVERSIONPOLICY.oneofs_by_name['policy_choice'].fields.append(
  _MODELVERSIONPOLICY.fields_by_name['latest'])
_MODELVERSIONPOLICY.fields_by_name['latest'].containing_oneof = _MODELVERSIONPOLICY.oneofs_by_name['policy_choice']
_MODELVERSIONPOLICY.oneofs_by_name['policy_choice'].fields.append(
  _MODELVERSIONPOLICY.fields_by_name['all'])
_MODELVERSIONPOLICY.fields_by_name['all'].containing_oneof = _MODELVERSIONPOLICY.oneofs_by_name['policy_choice']
_MODELVERSIONPOLICY.oneofs_by_name['policy_choice'].fields.append(
  _MODELVERSIONPOLICY.fields_by_name['specific'])
_MODELVERSIONPOLICY.fields_by_name['specific'].containing_oneof = _MODELVERSIONPOLICY.oneofs_by_name['policy_choice']
_MODELOPTIMIZATIONPOLICY_GRAPH.containing_type = _MODELOPTIMIZATIONPOLICY
_MODELOPTIMIZATIONPOLICY.fields_by_name['graph'].message_type = _MODELOPTIMIZATIONPOLICY_GRAPH
_MODELOPTIMIZATIONPOLICY.fields_by_name['priority'].enum_type = _MODELOPTIMIZATIONPOLICY_MODELPRIORITY
_MODELOPTIMIZATIONPOLICY_MODELPRIORITY.containing_type = _MODELOPTIMIZATIONPOLICY
_MODELSEQUENCEBATCHING_CONTROL.fields_by_name['kind'].enum_type = _MODELSEQUENCEBATCHING_CONTROL_KIND
_MODELSEQUENCEBATCHING_CONTROL.containing_type = _MODELSEQUENCEBATCHING
_MODELSEQUENCEBATCHING_CONTROL_KIND.containing_type = _MODELSEQUENCEBATCHING_CONTROL
_MODELSEQUENCEBATCHING_CONTROLINPUT.fields_by_name['control'].message_type = _MODELSEQUENCEBATCHING_CONTROL
_MODELSEQUENCEBATCHING_CONTROLINPUT.containing_type = _MODELSEQUENCEBATCHING
_MODELSEQUENCEBATCHING.fields_by_name['control_input'].message_type = _MODELSEQUENCEBATCHING_CONTROLINPUT
_MODELENSEMBLING_STEP_INPUTMAPENTRY.containing_type = _MODELENSEMBLING_STEP
_MODELENSEMBLING_STEP_OUTPUTMAPENTRY.containing_type = _MODELENSEMBLING_STEP
_MODELENSEMBLING_STEP.fields_by_name['input_map'].message_type = _MODELENSEMBLING_STEP_INPUTMAPENTRY
_MODELENSEMBLING_STEP.fields_by_name['output_map'].message_type = _MODELENSEMBLING_STEP_OUTPUTMAPENTRY
_MODELENSEMBLING_STEP.containing_type = _MODELENSEMBLING
_MODELENSEMBLING.fields_by_name['step'].message_type = _MODELENSEMBLING_STEP
_MODELCONFIG_CCMODELFILENAMESENTRY.containing_type = _MODELCONFIG
_MODELCONFIG_METRICTAGSENTRY.containing_type = _MODELCONFIG
_MODELCONFIG_PARAMETERSENTRY.fields_by_name['value'].message_type = _MODELPARAMETER
_MODELCONFIG_PARAMETERSENTRY.containing_type = _MODELCONFIG
_MODELCONFIG.fields_by_name['version_policy'].message_type = _MODELVERSIONPOLICY
_MODELCONFIG.fields_by_name['input'].message_type = _MODELINPUT
_MODELCONFIG.fields_by_name['output'].message_type = _MODELOUTPUT
_MODELCONFIG.fields_by_name['optimization'].message_type = _MODELOPTIMIZATIONPOLICY
_MODELCONFIG.fields_by_name['dynamic_batching'].message_type = _MODELDYNAMICBATCHING
_MODELCONFIG.fields_by_name['sequence_batching'].message_type = _MODELSEQUENCEBATCHING
_MODELCONFIG.fields_by_name['ensemble_scheduling'].message_type = _MODELENSEMBLING
_MODELCONFIG.fields_by_name['instance_group'].message_type = _MODELINSTANCEGROUP
_MODELCONFIG.fields_by_name['cc_model_filenames'].message_type = _MODELCONFIG_CCMODELFILENAMESENTRY
_MODELCONFIG.fields_by_name['metric_tags'].message_type = _MODELCONFIG_METRICTAGSENTRY
_MODELCONFIG.fields_by_name['parameters'].message_type = _MODELCONFIG_PARAMETERSENTRY
_MODELCONFIG.oneofs_by_name['scheduling_choice'].fields.append(
  _MODELCONFIG.fields_by_name['dynamic_batching'])
_MODELCONFIG.fields_by_name['dynamic_batching'].containing_oneof = _MODELCONFIG.oneofs_by_name['scheduling_choice']
_MODELCONFIG.oneofs_by_name['scheduling_choice'].fields.append(
  _MODELCONFIG.fields_by_name['sequence_batching'])
_MODELCONFIG.fields_by_name['sequence_batching'].containing_oneof = _MODELCONFIG.oneofs_by_name['scheduling_choice']
_MODELCONFIG.oneofs_by_name['scheduling_choice'].fields.append(
  _MODELCONFIG.fields_by_name['ensemble_scheduling'])
_MODELCONFIG.fields_by_name['ensemble_scheduling'].containing_oneof = _MODELCONFIG.oneofs_by_name['scheduling_choice']
DESCRIPTOR.message_types_by_name['ModelInstanceGroup'] = _MODELINSTANCEGROUP
DESCRIPTOR.message_types_by_name['ModelTensorReshape'] = _MODELTENSORRESHAPE
DESCRIPTOR.message_types_by_name['ModelInput'] = _MODELINPUT
DESCRIPTOR.message_types_by_name['ModelOutput'] = _MODELOUTPUT
DESCRIPTOR.message_types_by_name['ModelVersionPolicy'] = _MODELVERSIONPOLICY
DESCRIPTOR.message_types_by_name['ModelOptimizationPolicy'] = _MODELOPTIMIZATIONPOLICY
DESCRIPTOR.message_types_by_name['ModelDynamicBatching'] = _MODELDYNAMICBATCHING
DESCRIPTOR.message_types_by_name['ModelSequenceBatching'] = _MODELSEQUENCEBATCHING
DESCRIPTOR.message_types_by_name['ModelEnsembling'] = _MODELENSEMBLING
DESCRIPTOR.message_types_by_name['ModelParameter'] = _MODELPARAMETER
DESCRIPTOR.message_types_by_name['ModelConfig'] = _MODELCONFIG
DESCRIPTOR.enum_types_by_name['DataType'] = _DATATYPE
_sym_db.RegisterFileDescriptor(DESCRIPTOR)

ModelInstanceGroup = _reflection.GeneratedProtocolMessageType('ModelInstanceGroup', (_message.Message,), dict(
  DESCRIPTOR = _MODELINSTANCEGROUP,
  __module__ = 'model_config_pb2'
  # @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelInstanceGroup)
  ))
_sym_db.RegisterMessage(ModelInstanceGroup)

ModelTensorReshape = _reflection.GeneratedProtocolMessageType('ModelTensorReshape', (_message.Message,), dict(
  DESCRIPTOR = _MODELTENSORRESHAPE,
  __module__ = 'model_config_pb2'
  # @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelTensorReshape)
  ))
_sym_db.RegisterMessage(ModelTensorReshape)

ModelInput = _reflection.GeneratedProtocolMessageType('ModelInput', (_message.Message,), dict(
  DESCRIPTOR = _MODELINPUT,
  __module__ = 'model_config_pb2'
  # @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelInput)
  ))
_sym_db.RegisterMessage(ModelInput)

ModelOutput = _reflection.GeneratedProtocolMessageType('ModelOutput', (_message.Message,), dict(
  DESCRIPTOR = _MODELOUTPUT,
  __module__ = 'model_config_pb2'
  # @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelOutput)
  ))
_sym_db.RegisterMessage(ModelOutput)

ModelVersionPolicy = _reflection.GeneratedProtocolMessageType('ModelVersionPolicy', (_message.Message,), dict(

  Latest = _reflection.GeneratedProtocolMessageType('Latest', (_message.Message,), dict(
    DESCRIPTOR = _MODELVERSIONPOLICY_LATEST,
    __module__ = 'model_config_pb2'
    # @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelVersionPolicy.Latest)
    ))
  ,

  All = _reflection.GeneratedProtocolMessageType('All', (_message.Message,), dict(
    DESCRIPTOR = _MODELVERSIONPOLICY_ALL,
    __module__ = 'model_config_pb2'
    # @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelVersionPolicy.All)
    ))
  ,

  Specific = _reflection.GeneratedProtocolMessageType('Specific', (_message.Message,), dict(
    DESCRIPTOR = _MODELVERSIONPOLICY_SPECIFIC,
    __module__ = 'model_config_pb2'
    # @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelVersionPolicy.Specific)
    ))
  ,
  DESCRIPTOR = _MODELVERSIONPOLICY,
  __module__ = 'model_config_pb2'
  # @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelVersionPolicy)
  ))
_sym_db.RegisterMessage(ModelVersionPolicy)
_sym_db.RegisterMessage(ModelVersionPolicy.Latest)
_sym_db.RegisterMessage(ModelVersionPolicy.All)
_sym_db.RegisterMessage(ModelVersionPolicy.Specific)

ModelOptimizationPolicy = _reflection.GeneratedProtocolMessageType('ModelOptimizationPolicy', (_message.Message,), dict(

  Graph = _reflection.GeneratedProtocolMessageType('Graph', (_message.Message,), dict(
    DESCRIPTOR = _MODELOPTIMIZATIONPOLICY_GRAPH,
    __module__ = 'model_config_pb2'
    # @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelOptimizationPolicy.Graph)
    ))
  ,
  DESCRIPTOR = _MODELOPTIMIZATIONPOLICY,
  __module__ = 'model_config_pb2'
  # @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelOptimizationPolicy)
  ))
_sym_db.RegisterMessage(ModelOptimizationPolicy)
_sym_db.RegisterMessage(ModelOptimizationPolicy.Graph)

ModelDynamicBatching = _reflection.GeneratedProtocolMessageType('ModelDynamicBatching', (_message.Message,), dict(
  DESCRIPTOR = _MODELDYNAMICBATCHING,
  __module__ = 'model_config_pb2'
  # @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelDynamicBatching)
  ))
_sym_db.RegisterMessage(ModelDynamicBatching)

ModelSequenceBatching = _reflection.GeneratedProtocolMessageType('ModelSequenceBatching', (_message.Message,), dict(

  Control = _reflection.GeneratedProtocolMessageType('Control', (_message.Message,), dict(
    DESCRIPTOR = _MODELSEQUENCEBATCHING_CONTROL,
    __module__ = 'model_config_pb2'
    # @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelSequenceBatching.Control)
    ))
  ,

  ControlInput = _reflection.GeneratedProtocolMessageType('ControlInput', (_message.Message,), dict(
    DESCRIPTOR = _MODELSEQUENCEBATCHING_CONTROLINPUT,
    __module__ = 'model_config_pb2'
    # @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelSequenceBatching.ControlInput)
    ))
  ,
  DESCRIPTOR = _MODELSEQUENCEBATCHING,
  __module__ = 'model_config_pb2'
  # @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelSequenceBatching)
  ))
_sym_db.RegisterMessage(ModelSequenceBatching)
_sym_db.RegisterMessage(ModelSequenceBatching.Control)
_sym_db.RegisterMessage(ModelSequenceBatching.ControlInput)

ModelEnsembling = _reflection.GeneratedProtocolMessageType('ModelEnsembling', (_message.Message,), dict(

  Step = _reflection.GeneratedProtocolMessageType('Step', (_message.Message,), dict(

    InputMapEntry = _reflection.GeneratedProtocolMessageType('InputMapEntry', (_message.Message,), dict(
      DESCRIPTOR = _MODELENSEMBLING_STEP_INPUTMAPENTRY,
      __module__ = 'model_config_pb2'
      # @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelEnsembling.Step.InputMapEntry)
      ))
    ,

    OutputMapEntry = _reflection.GeneratedProtocolMessageType('OutputMapEntry', (_message.Message,), dict(
      DESCRIPTOR = _MODELENSEMBLING_STEP_OUTPUTMAPENTRY,
      __module__ = 'model_config_pb2'
      # @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelEnsembling.Step.OutputMapEntry)
      ))
    ,
    DESCRIPTOR = _MODELENSEMBLING_STEP,
    __module__ = 'model_config_pb2'
    # @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelEnsembling.Step)
    ))
  ,
  DESCRIPTOR = _MODELENSEMBLING,
  __module__ = 'model_config_pb2'
  # @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelEnsembling)
  ))
_sym_db.RegisterMessage(ModelEnsembling)
_sym_db.RegisterMessage(ModelEnsembling.Step)
_sym_db.RegisterMessage(ModelEnsembling.Step.InputMapEntry)
_sym_db.RegisterMessage(ModelEnsembling.Step.OutputMapEntry)

ModelParameter = _reflection.GeneratedProtocolMessageType('ModelParameter', (_message.Message,), dict(
  DESCRIPTOR = _MODELPARAMETER,
  __module__ = 'model_config_pb2'
  # @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelParameter)
  ))
_sym_db.RegisterMessage(ModelParameter)

ModelConfig = _reflection.GeneratedProtocolMessageType('ModelConfig', (_message.Message,), dict(

  CcModelFilenamesEntry = _reflection.GeneratedProtocolMessageType('CcModelFilenamesEntry', (_message.Message,), dict(
    DESCRIPTOR = _MODELCONFIG_CCMODELFILENAMESENTRY,
    __module__ = 'model_config_pb2'
    # @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelConfig.CcModelFilenamesEntry)
    ))
  ,

  MetricTagsEntry = _reflection.GeneratedProtocolMessageType('MetricTagsEntry', (_message.Message,), dict(
    DESCRIPTOR = _MODELCONFIG_METRICTAGSENTRY,
    __module__ = 'model_config_pb2'
    # @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelConfig.MetricTagsEntry)
    ))
  ,

  ParametersEntry = _reflection.GeneratedProtocolMessageType('ParametersEntry', (_message.Message,), dict(
    DESCRIPTOR = _MODELCONFIG_PARAMETERSENTRY,
    __module__ = 'model_config_pb2'
    # @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelConfig.ParametersEntry)
    ))
  ,
  DESCRIPTOR = _MODELCONFIG,
  __module__ = 'model_config_pb2'
  # @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelConfig)
  ))
_sym_db.RegisterMessage(ModelConfig)
_sym_db.RegisterMessage(ModelConfig.CcModelFilenamesEntry)
_sym_db.RegisterMessage(ModelConfig.MetricTagsEntry)
_sym_db.RegisterMessage(ModelConfig.ParametersEntry)


_MODELENSEMBLING_STEP_INPUTMAPENTRY._options = None
_MODELENSEMBLING_STEP_OUTPUTMAPENTRY._options = None
_MODELCONFIG_CCMODELFILENAMESENTRY._options = None
_MODELCONFIG_METRICTAGSENTRY._options = None
_MODELCONFIG_PARAMETERSENTRY._options = None
# @@protoc_insertion_point(module_scope)
